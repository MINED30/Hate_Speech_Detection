{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HATESPEECH_07_(Modeling_with_FinalDataset_01).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DiNDRrVbwuM7"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUXEhOy_-Kqp",
        "outputId": "cff482dd-74ce-4c11-da14-9d12ef74158c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  3 19:57:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    31W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtiIAyz6siMa"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l0sbvXs2xEQ"
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtVTu1IFpNRq",
        "outputId": "8203981a-f166-4a11-8bcc-52b3517f579b"
      },
      "source": [
        "# Train dataset / Validation dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/HateSpeech/FINAL_DATASET/Final_dataset_balanced.csv\")\n",
        "df = df.dropna()\n",
        "df_train, df_val = train_test_split(df,test_size=0.2,random_state = 42)\n",
        "# Test dataset\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/HateSpeech/hateXplain.csv\")\n",
        "print(df_train.shape,df_val.shape,df_test.shape)\n",
        "df_train = df_train[(df_train['text'].apply(len)<1000)]\n",
        "df_val = df_val[(df_val['text'].apply(len)<1000)]\n",
        "df_test = df_test[(df_test['text'].apply(len)<1000)]\n",
        "print(df_train.shape,df_val.shape,df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(123424, 3) (30856, 3) (15351, 3)\n",
            "(119868, 3) (29931, 3) (15351, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lzkIIJtwWTX"
      },
      "source": [
        "X_train = df_train['text']\n",
        "y_train = df_train['class']\n",
        "X_val = df_val['text']\n",
        "y_val = df_val['class']\n",
        "X_test = df_test['text']\n",
        "y_test = df_test['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ii4zdiSwrfu"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiNDRrVbwuM7"
      },
      "source": [
        "## GloVe + BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQjqP2F3wt5L"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import *\n",
        "\n",
        "def build_model(num_words,embedding_matrix):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(num_words,\n",
        "                      embedding_dim,\n",
        "                      embeddings_initializer=Constant(embedding_matrix),\n",
        "                      input_length=sequence_length,\n",
        "                      trainable=True))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(32)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6xslX_6xh72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfc0049-986a-46f6-a5fb-d50c31334e2d"
      },
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open(\"/content/drive/MyDrive/HateSpeech/glove.6B.100d.txt\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p3PbLwdw3hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb86165-d2b6-4c0a-9388-c2757b01a4fd"
      },
      "source": [
        "X_train = df_train['text']\n",
        "y_train = df_train['class']\n",
        "X_val = df_val['text']\n",
        "y_val = df_val['class']\n",
        "X_test = df_test['text']\n",
        "y_test = df_test['class']\n",
        "\n",
        "eval = pd.DataFrame([[np.nan for i in range(11)]])\n",
        "eval.columns = ['Model',\n",
        "                'Train_Score(ACC)','Train_Score(ROC_AUC)','Train_Score(F1)',\n",
        "                'Val_Score(ACC)','Val_Score(ROC_AUC)','Val_Score(F1)',\n",
        "                'Test_Score(ACC)','Test_Score(ROC_AUC)','Test_Score(F1)','Inference_Time']\n",
        "eval = eval.iloc[1:]\n",
        "\n",
        "# Initialization\n",
        "max_features=100000\n",
        "sequence_length = 235\n",
        "embedding_dim = 100\n",
        "num_words = 100001\n",
        "\n",
        "# Tokenizing\n",
        "data_start = time.time()\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, sequence_length)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix_train = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_train[i] = embedding_vector\n",
        "    else:\n",
        "        embedding_matrix_train[i] = np.random.randn(embedding_dim)\n",
        "\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_val = pad_sequences(X_val, sequence_length)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, sequence_length)\n",
        "\n",
        "# Fit\n",
        "LR = build_model(num_words,embedding_matrix_train)\n",
        "filepath = \"/content/drive/MyDrive/HateSpeech/Weight/GloVe_BiLSTM\"\n",
        "best_weight = tf.keras.callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=0, save_best_only=True,save_weights_only=True)\n",
        "early = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=1, verbose=3, mode='auto', baseline=None, restore_best_weights=False )\n",
        "LR.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=1000,batch_size=256,callbacks=[early,best_weight])\n",
        "LR.load_weights(filepath)\n",
        "\n",
        "# Inference\n",
        "print(\"TRAIN SET\")\n",
        "fitted = LR.predict_classes(X_train)\n",
        "fitted_proba = LR.predict_proba(X_train)\n",
        "\n",
        "print(\"VAL SET\")\n",
        "val_pred = LR.predict_classes(X_val)\n",
        "val_pred_proba = LR.predict_proba(X_val)\n",
        "\n",
        "print(\"TEST SET\")\n",
        "start = time.time()\n",
        "test_pred = LR.predict_classes(X_test)\n",
        "inference_time = time.time()-start\n",
        "test_pred_proba = LR.predict_proba(X_test)\n",
        "print(f\"Inferenced : {inference_time}s\",end='\\t')\n",
        "\n",
        "# Evaluate\n",
        "train_acc = accuracy_score(y_train,fitted)\n",
        "train_auc = roc_auc_score(y_train,fitted_proba)\n",
        "train_f1 = f1_score(y_train,fitted)\n",
        "\n",
        "val_acc = accuracy_score(y_val,val_pred)\n",
        "val_auc = roc_auc_score(y_val,val_pred_proba)\n",
        "val_f1 = f1_score(y_val,val_pred)\n",
        "\n",
        "test_acc = accuracy_score(y_test,test_pred)\n",
        "test_auc = roc_auc_score(y_test,test_pred_proba)\n",
        "test_f1 = f1_score(y_test,test_pred)\n",
        "print(f\"train ACC : {train_acc} train F1 : {train_f1} test ACC : {test_acc} test ROCAUC : {test_auc} test F1 : {test_f1}\")\n",
        "\n",
        "LR_list = ['BiLSTM+GloVe(10)']\n",
        "LR_list.append(train_acc)\n",
        "LR_list.append(train_auc)\n",
        "LR_list.append(train_f1)\n",
        "LR_list.append(val_acc)\n",
        "LR_list.append(val_auc)\n",
        "LR_list.append(val_f1)\n",
        "LR_list.append(test_acc)\n",
        "LR_list.append(test_auc)\n",
        "LR_list.append(test_f1)\n",
        "LR_list.append(inference_time)\n",
        "\n",
        "eval = eval.append(pd.DataFrame([LR_list],columns=eval.columns))\n",
        "eval.to_csv(f\"/content/drive/MyDrive/HateSpeech/PERFORMANCE2/TEST_GloVe_BiLSTM.csv\")\n",
        "print(f\"SAVED!!! {time.time()-data_start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "469/469 [==============================] - 62s 121ms/step - loss: 0.4773 - accuracy: 0.7631 - val_loss: 0.3341 - val_accuracy: 0.8535\n",
            "Epoch 2/1000\n",
            "469/469 [==============================] - 55s 117ms/step - loss: 0.3169 - accuracy: 0.8625 - val_loss: 0.2897 - val_accuracy: 0.8770\n",
            "Epoch 3/1000\n",
            "469/469 [==============================] - 55s 118ms/step - loss: 0.2695 - accuracy: 0.8863 - val_loss: 0.2845 - val_accuracy: 0.8795\n",
            "Epoch 4/1000\n",
            "469/469 [==============================] - 55s 118ms/step - loss: 0.2411 - accuracy: 0.9001 - val_loss: 0.2858 - val_accuracy: 0.8805\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa3ec4c4910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdLC3vdN1ri0"
      },
      "source": [
        "## BERT Family (Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Jx_i44-W60"
      },
      "source": [
        "### Load Tokenizer & Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRzXlDsm1uV5"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf54LIpJ2a2e",
        "outputId": "c20adc8a-9b44-4afa-defe-63d61c424a23"
      },
      "source": [
        "from transformers import ElectraTokenizer, TFElectraForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
        "from transformers import MobileBertTokenizer, TFMobileBertForSequenceClassification\n",
        "from transformers import AlbertTokenizerFast, TFAlbertForSequenceClassification\n",
        "from transformers import AutoTokenizer, TFMobileBertForSequenceClassification, TFAutoModelForSequenceClassification, TFAutoModel\n",
        "\n",
        "Model_list = []\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/xtremedistil-l12-h384-uncased\")\n",
        "model = TFAutoModel.from_pretrained(\"microsoft/xtremedistil-l12-h384-uncased\")\n",
        "Model_list.append((tokenizer,model,'MobileBERT','google/mobilebert-uncased','TF'))\n",
        " \n",
        "tokenizer_electra = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
        "model_electra = TFElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n",
        "Model_list.append((tokenizer_electra,model_electra,'ELECTRA','google/electra-small-discriminator','TF'))\n",
        " \n",
        "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model_distilbert = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "Model_list.append((tokenizer_distilbert,model_distilbert,'DistilBERT','distilbert-base-uncased','TF'))\n",
        " \n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model_roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "Model_list.append((tokenizer_roberta,model_roberta,'RoBERTa','roberta-base','TF'))\n",
        "\n",
        "tokenizer_mobilebert = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
        "model_mobilebert = TFMobileBertForSequenceClassification.from_pretrained(\"google/mobilebert-uncased\")\n",
        "Model_list.append((tokenizer_mobilebert,model_mobilebert,'MobileBERT','google/mobilebert-uncased','TF'))\n",
        "\n",
        "\n",
        "tokenizer_twitter = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate\")\n",
        "model_twitter = TFAutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate\")\n",
        "Model_list.append((tokenizer_twitter,model_twitter,'twitter-roberta',\"cardiffnlp/twitter-roberta-base-hate\",'TF'))\n",
        "\n",
        "tokenizer_mobilebert = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
        "model_mobilebert = TFMobileBertForSequenceClassification.from_pretrained('google/mobilebert-uncased')\n",
        "Model_list.append((tokenizer_mobilebert,model_mobilebert,'MobileBERT','google/mobilebert-uncased','TF'))\n",
        " \n",
        "tokenizer_albert = AlbertTokenizerFast.from_pretrained('albert-base-v2')\n",
        "model_albert = TFAlbertForSequenceClassification.from_pretrained('albert-base-v2')\n",
        "Model_list.append((tokenizer_albert,model_albert,'ALBERT','albert-base-v2','TF'))\n",
        " \n",
        "for i in range(len(Model_list)):\n",
        "  print(f\"{i+1} - Tokenizer[0] : {str(Model_list[i][0].__class__).split('.')[-1][:-2]}\",end='\\t\\t')\n",
        "  print(f\"Model[1] : {str(Model_list[i][1].__class__).split('.')[-1][:-2]:38s}\",end='\\t')\n",
        "  print(f\"Name[2] : {Model_list[i][2]:10s}\",end='\\t')\n",
        "  print(f\"Pretrained[3] : {Model_list[i][3]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFMobileBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFMobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-hate.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 - Tokenizer[0] : MobileBertTokenizerFast\t\tModel[1] : TFMobileBertForSequenceClassification \tName[2] : MobileBERT\tPretrained[3] : google/mobilebert-uncased\n",
            "2 - Tokenizer[0] : RobertaTokenizerFast\t\tModel[1] : TFRobertaForSequenceClassification    \tName[2] : twitter-roberta\tPretrained[3] : cardiffnlp/twitter-roberta-base-hate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqNp0fwN-bk_"
      },
      "source": [
        "### Train & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YSnSlFNo29kd",
        "outputId": "053df0aa-36ec-4c7e-9621-f8395db2989b"
      },
      "source": [
        "X_train = df_train['text']\n",
        "y_train = df_train['class']\n",
        "X_val = df_val['text']\n",
        "y_val = df_val['class']\n",
        "X_test = df_test['text']\n",
        "y_test = df_test['class']\n",
        "\n",
        "X_train = X_train.to_list()\n",
        "X_val = X_val.to_list()\n",
        "X_test = X_test.to_list()\n",
        "y_train = y_train.to_list()\n",
        "y_val = y_val.to_list()\n",
        "y_test = y_test.to_list()\n",
        " \n",
        "for tokenizer, model_seq, model_name, pretrained, plat in Model_list:\n",
        "  data_start = time.time()\n",
        "  eval = pd.DataFrame([[np.nan for i in range(11)]])\n",
        "  eval.columns = ['Model',\n",
        "                  'Train_Score(ACC)','Train_Score(ROC_AUC)','Train_Score(F1)',\n",
        "                  'Val_Score(ACC)','Val_Score(ROC_AUC)','Val_Score(F1)',\n",
        "                  'Test_Score(ACC)','Test_Score(ROC_AUC)','Test_Score(F1)','Inference_Time']\n",
        "  eval = eval.iloc[1:]\n",
        "  print(f\"Tokenizer[0] : {str(tokenizer.__class__).split('.')[-1][:-2]}\",end='\\t\\t')\n",
        "  print(f\"Model[1] : {str(model_seq.__class__).split('.')[-1][:-2]:38s}\",end='\\t')\n",
        "  print(f\"Name[2] : {model_name:10s}\",end='\\t')\n",
        "  print(f\"Pretrained[3] : {pretrained}\")\n",
        " \n",
        "  # Initialization\n",
        "  LR = model_seq\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "  LR.compile(optimizer=optimizer, loss='binary_crossentropy') # can also use any keras loss fn\n",
        " \n",
        "  # Tokenizing\n",
        "  encoding_time = time.time()\n",
        "  train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
        "  val_encodings = tokenizer(X_val, truncation=True, padding=True)\n",
        "  test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
        "  print(\"encoding time : \", time.time()-encoding_time)\n",
        " \n",
        "  # Build Dataset\n",
        "  dataset_time = time.time()\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      dict(train_encodings),\n",
        "      y_train\n",
        "  ))\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      dict(val_encodings),\n",
        "      y_val\n",
        "  ))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      dict(test_encodings),\n",
        "      y_test\n",
        "  ))\n",
        "  print(\"building dataset : \", time.time()-dataset_time)\n",
        " \n",
        "  # Fit\n",
        "  filepath = f\"/content/drive/MyDrive/HateSpeech/Weight/{model_name}\"\n",
        "  best_weight = tf.keras.callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
        "  early = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=3, verbose=3, mode='auto', baseline=None, restore_best_weights=False )\n",
        "  LR.fit(train_dataset.shuffle(1000).batch(16),\n",
        "         validation_data=val_dataset.shuffle(1000).batch(16),\n",
        "         epochs=1000,\n",
        "         callbacks=[early,best_weight])\n",
        " \n",
        "  # Inference\n",
        "  fitted = LR.predict(train_dataset.batch(16))\n",
        "  fitted = tf.nn.softmax(fitted.logits, axis=1)\n",
        "  fitted_proba = fitted[:,1]\n",
        "  fitted = [np.argmax(res) for res in fitted]\n",
        " \n",
        "  val_pred = LR.predict(val_dataset.batch(16))\n",
        "  val_pred = tf.nn.softmax(val_pred.logits, axis=1)\n",
        "  val_pred_proba = val_pred[:,1]\n",
        "  val_pred = [np.argmax(res) for res in val_pred]\n",
        " \n",
        "  start = time.time()\n",
        "  test_pred = LR.predict(test_dataset.batch(16))\n",
        "  test_pred = tf.nn.softmax(test_pred.logits, axis=1)\n",
        "  inference_time = time.time()-start\n",
        "  test_pred_proba = test_pred[:,1]\n",
        "  test_pred = [np.argmax(res) for res in test_pred]\n",
        "  print(f\"Inferenced : {inference_time}s\",end='\\t')\n",
        " \n",
        "  # Evaluate\n",
        "  train_acc = accuracy_score(y_train,fitted)\n",
        "  train_auc = roc_auc_score(y_train,fitted_proba)\n",
        "  train_f1 = f1_score(y_train,fitted)\n",
        " \n",
        "  val_acc = accuracy_score(y_val,val_pred)\n",
        "  val_auc = roc_auc_score(y_val,val_pred_proba)\n",
        "  val_f1 = f1_score(y_val,val_pred)\n",
        " \n",
        "  test_acc = accuracy_score(y_test,test_pred)\n",
        "  test_auc = roc_auc_score(y_test,test_pred_proba)\n",
        "  test_f1 = f1_score(y_test,test_pred)\n",
        " \n",
        "  print(f\"TRAIN ROC_AUC : {train_auc} VAL ROC_AUC : {val_auc} TEST ROC_AUC : {test_auc}\")\n",
        " \n",
        " \n",
        "  LR_list = [model_name]\n",
        "  LR_list.append(train_acc)\n",
        "  LR_list.append(train_auc)\n",
        "  LR_list.append(train_f1)\n",
        "  LR_list.append(val_acc)\n",
        "  LR_list.append(val_auc)\n",
        "  LR_list.append(val_f1)\n",
        "  LR_list.append(test_acc)\n",
        "  LR_list.append(test_auc)\n",
        "  LR_list.append(test_f1)\n",
        "  LR_list.append(inference_time)\n",
        " \n",
        "  eval = eval.append(pd.DataFrame([LR_list],columns=eval.columns))\n",
        "  eval.to_csv(f\"/content/drive/MyDrive/HateSpeech/PERFORMANCE2/{model_name}.csv\")\n",
        "  print(f\"SAVED!!! {time.time()-data_start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenizer[0] : MobileBertTokenizerFast\t\tModel[1] : TFMobileBertForSequenceClassification \tName[2] : MobileBERT\tPretrained[3] : google/mobilebert-uncased\n",
            "encoding time :  19.308887004852295\n",
            "building dataset :  480.92417907714844\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f66dd984ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f66dd984ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f6709235dd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f6709235dd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "7492/7492 [==============================] - ETA: 0s - loss: 7.6693WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "7492/7492 [==============================] - 2908s 376ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 5315). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/HateSpeech/Weight/MobileBERT/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/HateSpeech/Weight/MobileBERT/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/1000\n",
            "7492/7492 [==============================] - 2801s 374ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 5315). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/HateSpeech/Weight/MobileBERT/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/HateSpeech/Weight/MobileBERT/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/1000\n",
            "7492/7492 [==============================] - 2804s 374ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "Epoch 4/1000\n",
            "7492/7492 [==============================] - 2809s 375ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 5315). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/HateSpeech/Weight/MobileBERT/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/HateSpeech/Weight/MobileBERT/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/1000\n",
            "7492/7492 [==============================] - 2804s 374ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "Epoch 6/1000\n",
            "7492/7492 [==============================] - 2809s 375ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "Epoch 7/1000\n",
            "7492/7492 [==============================] - 2808s 375ms/step - loss: 7.6693 - val_loss: 7.6692\n",
            "Epoch 00007: early stopping\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inferenced : 128.82943558692932s\tTRAIN ROC_AUC : 0.5 VAL ROC_AUC : 0.5 TEST ROC_AUC : 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SAVED!!! 22519.817007541656\n",
            "Tokenizer[0] : RobertaTokenizerFast\t\tModel[1] : TFRobertaForSequenceClassification    \tName[2] : twitter-roberta\tPretrained[3] : cardiffnlp/twitter-roberta-base-hate\n",
            "encoding time :  27.990967273712158\n",
            "building dataset :  358.93810081481934\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a099b4899c2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m          callbacks=[early,best_weight])\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[16,12,478,478] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/Softmax (defined at /usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py:247) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1359598]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/Softmax:\n tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/Add (defined at /usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py:244)\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kovlcf-QlzT"
      },
      "source": [
        "\"https://zzsza.github.io/mlops/2021/04/18/bentoml-basic/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMqd0ASn4lkJ"
      },
      "source": [
        "## BERT Family (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC-KuBGA4rGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e8645f-483c-4ad0-ecec-0627fc193896"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
        "Model_list = []\n",
        "\n",
        "tokenizer_xtremedistil = AutoTokenizer.from_pretrained(\"microsoft/xtremedistil-l12-h384-uncased\")\n",
        "model_xtremedistil = AutoModel.from_pretrained(\"microsoft/xtremedistil-l12-h384-uncased\")\n",
        "Model_list.append((tokenizer_xtremedistil,model_xtremedistil,'xtremedistil','xtremedistil-l12-h384-uncased','TF'))\n",
        "  \n",
        "tokenizer_squeeze = AutoTokenizer.from_pretrained(\"squeezebert/squeezebert-uncased\")\n",
        "model_squeeze = AutoModel.from_pretrained(\"squeezebert/squeezebert-uncased\")\n",
        "Model_list.append((tokenizer_squeeze,model_squeeze,'squeezebert',\"squeezebert/squeezebert-uncased\",'TF'))\n",
        "\n",
        "tokenizer_dehatebert = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
        "model_dehatebert = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
        "Model_list.append((tokenizer_dehatebert,model_dehatebert,'squeezebert',\"squeezebert/squeezebert-uncased\",'TF'))\n",
        "\n",
        "tokenizer_tweetroberta = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-offensive\")\n",
        "model_tweetroberta = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-offensive\")\n",
        "Model_list.append((tokenizer_tweetroberta,model_tweetroberta,'squeezebert',\"squeezebert/squeezebert-uncased\",'TF'))\n",
        "\n",
        "for i in range(len(Model_list)):\n",
        "  print(f\"{i+1} - Tokenizer[0] : {str(Model_list[i][0].__class__).split('.')[-1][:-2]}\",end='\\t\\t')\n",
        "  print(f\"Model[1] : {str(Model_list[i][1].__class__).split('.')[-1][:-2]:38s}\",end='\\t')\n",
        "  print(f\"Name[2] : {Model_list[i][2]:10s}\",end='\\t')\n",
        "  print(f\"Pretrained[3] : {Model_list[i][3]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 - Tokenizer[0] : BertTokenizerFast\t\tModel[1] : BertModel                             \tName[2] : xtremedistil\tPretrained[3] : xtremedistil-l12-h384-uncased\n",
            "2 - Tokenizer[0] : SqueezeBertTokenizerFast\t\tModel[1] : SqueezeBertModel                      \tName[2] : squeezebert\tPretrained[3] : squeezebert/squeezebert-uncased\n",
            "3 - Tokenizer[0] : BertTokenizerFast\t\tModel[1] : BertForSequenceClassification         \tName[2] : squeezebert\tPretrained[3] : squeezebert/squeezebert-uncased\n",
            "4 - Tokenizer[0] : RobertaTokenizerFast\t\tModel[1] : RobertaForSequenceClassification      \tName[2] : squeezebert\tPretrained[3] : squeezebert/squeezebert-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}